# Awesome-Neuromorphic-Navigation-Resources [![Awesome](https://awesome.re/badge-flat.svg)](https://awesome.re)

This is a repository of **Neuromorphic Navigation** survey from **Southeast University, Nanjing, China**. 

It aims to provide a curated collection of resources for researchers and practitioners interested in neuromorphic navigation, including datasets, hardware platforms, and software tools.

If you find this repository helpful, please consider Stars ‚≠ê or Sharing üëã.

## Data Resources

### Real-World Datasets
|      Dataset       |     Pub.     | Paper                                                                                                                                                                                  |                             Open Resource                              |
|:------------------:|:------------:|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------:|
|     DAVIS240C      |  IJRR-2017   | [The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM](https://journals.sagepub.com/doi/10.1177/0278364917691115)                   |           [Website](https://rpg.ifi.uzh.ch/davis_data.html)            |
|       MVSEC        |   RAL-2018   | [The Multivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception](https://ieeexplore.ieee.org/document/8288670)                                                |          [Website](https://daniilidis-group.github.io/mvsec/)          |
|        DSEC        |   RAL-2021   | [DSEC: A Stereo Event Camera Dataset for Driving Scenarios](https://ieeexplore.ieee.org/document/9387069)                                                                              |               [GitHub](https://github.com/uzh-rpg/DSEC)                |
|       M2DGR        |   RAL-2022   | [M2DGR: A Multi-Sensor and Multi-Scenario SLAM Dataset for Ground Robots](https://ieeexplore.ieee.org/document/9664374)                                                                |             [GitHub](https://github.com/SJTU-ViSYS/M2DGR)              |
|      UZH-FPV       |  ICRA-2019   | [Are We Ready for Autonomous Drone Racing? The UZH-FPV Drone Racing Dataset](https://ieeexplore.ieee.org/document/8793887)                                                             |                   [Website](https://fpv.ifi.uzh.ch/)                   |
|       D-eDVS       |  ICRA-2014   | [Event-based 3D SLAM with a depth-augmented dynamic vision sensor](https://ieeexplore.ieee.org/document/6906882)                                                                       |                                   -                                    |
|       DDD17        |  ICML-2017   | [DDD17: End-To-End DAVIS Driving Dataset](https://arxiv.org/abs/1711.01458)                                                                                                            |                [Website](sensors.ini.uzh.ch/databases)                 |
|       DDD20        |  ITSC-2020   | [DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events with Deep Learning for Improved Steering Prediction](https://ieeexplore.ieee.org/document/9294515)            |          [Website](http://sensors.ini.uzh.ch/databases.html)           |
|       VECtor       |   RAL-2022   | [VECtor: A Versatile Event-Centric Benchmark for Multi-Sensor SLAM](https://ieeexplore.ieee.org/document/9809788)                                                                      |      [Website](https://star-datasets.github.io/vector/download/)       |
|      ViViD++       |   RAL-2022   | [ViViD++ : Vision for Visibility Dataset](https://ieeexplore.ieee.org/document/9760091)                                                                                                |            [Website](https://visibilitydataset.github.io/)             |
|      TUM-VIE       |  IROS-2021   | [TUM-VIE: The TUM Stereo Visual-Inertial Event Dataset](https://ieeexplore.ieee.org/document/9636728)                                                                                  |      [Website](https://service.tib.eu/ldmservice/dataset/tum-vie)      |
| Brisbane-Event-VPR |   RAL-2020   | [Event-Based Visual Place Recognition With Ensembles of Temporal Windows](https://ieeexplore.ieee.org/document/9201344)                                                                |     [GitHub](https://github.com/Tobias-Fischer/ensemble-event-vpr)     |
|      GRIFFIN       |   RAL-2021   | [The GRIFFIN Perception Dataset: Bridging the Gap Between Flapping-Wing Flight and Robotic Perception](https://ieeexplore.ieee.org/document/9345513)                                   |             [Website](https://zenodo.org/records/11404043)             |
|      AGRI-EBV      |  ICRA-2021   | [An Event-based Vision Dataset for Visual Navigation Tasks in Agricultural Environments](https://ieeexplore.ieee.org/document/9561741)                                                 |    [Website](https://ieee-dataport.org/open-access/agri-ebv-autumn)    |
|   FusionPortable   |  IROS-2022   | [FusionPortable: A Multi-Sensor Campus-Scene Dataset for Evaluation of Localization and Mapping Accuracy on Diverse Platforms](https://ieeexplore.ieee.org/abstract/document/9982119)  |  [Website](https://fusionportable.github.io/dataset/fusionportable/)   |
|        M3ED        |  CVPRW-2023  | [M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset](https://ieeexplore.ieee.org/document/10209006)                                                                      |           [GitHub](https://github.com/daniilidis-group/m3ed)           |
|  Hadviger et al.   |  ECMR-2023   | [Stereo Visual Localization Dataset Featuring Event Cameras](https://ieeexplore.ieee.org/document/10256407)                                                                            |   [Website](http://www.bitbucket.com/unizg-fer-lamor/event-dataset)    |
|   PKU-DAVIS-SOD    |  TPAMI-2023  | [SODFormer: Streaming Object Detection With Transformer Using Events and Frames](https://ieeexplore.ieee.org/document/10195232)                                                        |  [Website](https://www.pkuml.org/research/pku-davis-sod-dataset.html)  |
|        ECMD        |   TIV-2024   | [ECMD: An Event-Centric Multisensory Driving Dataset for SLAM](https://ieeexplore.ieee.org/abstract/document/10342726)                                                                 |              [GitHub](https://github.com/arclab-hku/ecmd)              |
|      MA-VIED       |  TITS-2024   | [MA-VIED: A Multisensor Automotive Visual Inertial Event Dataset](https://ieeexplore.ieee.org/abstract/document/10254473)                                                              |    [GitHub](https://github.com/isarlab-department-engineering/MA-VIED)    |
|        CEAR        |   RAL-2024   | [CEAR: Comprehensive Event Camera Dataset for Rapid Perception of Agile Quadruped Robots](https://ieeexplore.ieee.org/document/10592643)                                               |               [GitHub](https://github.com/DARoSLab/cear)               |
|       NSAVP        |  IJRR-2025   | [Dataset and Benchmark: Novel Sensors for Autonomous Vehicle Perception](https://doi.org/10.1177/02783649241273554)                                                                    |             [Website](https://umautobots.github.io/nsavp)              |
|        M2P2        |  ArXiv-2025  | [M2P2: A Multi-Modal Passive Perception Dataset for Off-Road Mobility in Extreme Low-Light Conditions](https://arxiv.org/abs/2410.01105)                                               |           [Website](https://cs.gmu.edu/~xiao/Research/M2P2/)           |
|       M3DSS        |  ICRA-2025   | [M3DSS: A Multi-Platform, Multi-Sensor, and Multi-Scenario Dataset for SLAM System](https://ieeexplore.ieee.org/document/11128631)                                                     |        [GitHub](https://github.com/NEUFS-MA/M3DSS)         |
|   NYC-Event-VPR    |  ICRA-2025   | [NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place Recognition Dataset in Dense Urban Environments](https://ieeexplore.ieee.org/document/11128789)                 |            [GitHub](https://github.com/ai4ce/NYC-Event-VPR)            |
|  FusionPortableV2  |  IJRR-2025   | [FusionPortableV2: A unified multi-sensor dataset for generalized SLAM across diverse platforms and scalable environments](https://journals.sagepub.com/doi/10.1177/02783649241303525) | [Website](https://fusionportable.github.io/dataset/fusionportable_v2/) |


### Event Simulators and Simulated Datasets
#### Event Simulators

|     Simulator     |              Pub.              | Paper                                                                                                                                                         |                      Open Resource                      |
|:-----------------:|:------------------------------:|---------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------:|
|       ESIM        |           CoRL-2018            | [ESIM: an Open Event Camera Simulator](https://rpg.ifi.uzh.ch/docs/CORL18_Rebecq.pdf)                                                                         |      [GitHub](https://github.com/uzh-rpg/rpg_esim)      |
|        v2e        |           CVPRW-2021           | [v2e: From Video Frames to Realistic DVS Events](https://ieeexplore.ieee.org/document/9523069)                                                                |       [GitHub](https://github.com/SensorsINI/v2e)       |
| DVS Gazebo Plugin |          SIMPAR-2016           | [Towards a framework for end-to-end control of a simulated vehicle with spiking neural networks](https://ieeexplore.ieee.org/document/7862386)                |                            -                            |
|      PIX2NVS      |           ICIP-2017            | [PIX2NVS: Parameterized conversion of pixel-domain video frames to neuromorphic vision streams](https://ieeexplore.ieee.org/document/8296630)                 |      [GitHub](https://github.com/PIX2NVS/PIX2NVS)       |
|       pyDVS       |           SSCI-2016            | [pyDVS: An extensible, real-time Dynamic Vision Sensor emulator using off-the-shelf hardware](https://ieeexplore.ieee.org/document/7850249)                   |       [GitHub](https://github.com/chanokin/pyDVS)       |
|     EventGAN      |           ICCP-2021            | [EventGAN: Leveraging Large Scale Image Datasets for Event Cameras](https://ieeexplore.ieee.org/abstract/document/9466265)                                    |     [GitHub](https://github.com/alexzzhu/EventGAN)      |
|       ViSim       |           ArXiv-2018           | [InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset](https://arxiv.org/abs/1809.00716)                                                |           [Website](https://interiornet.org/)           |
|       VID2E       |           CVPR-2020            | [Video to Events: Recycling Video Datasets for Event Cameras](https://ieeexplore.ieee.org/document/9156296)                                                   |     [GitHub](https://github.com/uzh-rpg/rpg_vid2e)      |
|       ICNS        | Frontiers in Neuroscience-2021 | [Event Camera Simulator Improvements via Characterized Parameters](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2021.702765/full) | [GitHub](https://github.com/neuromorphicsystems/IEBCS)  |
|       VISTA       |           ICRA-2022            | [VISTA 2.0: An Open, Data-driven Simulator for Multimodal Sensing and Policy Learning for Autonomous Vehicles](https://ieeexplore.ieee.org/document/9812276)  |   [GitHub](https://github.com/vista-simulator/vista)    |
|   DVS-Voltmeter   |           ECCV-2022            | [DVS-Voltmeter: Stochastic Process-Based Event Simulator for Dynamic Vision Sensors](https://link.springer.com/chapter/10.1007/978-3-031-20071-7_34)          |   [GitHub](https://github.com/Lynn0306/DVS-Voltmeter)   |
|       V2CE        |           ICRA-2024            | [V2CE: Video to Continuous Events Simulator](https://ieeexplore.ieee.org/document/10609864)                                                                   | [GitHub](https://github.com/ucsd-hdsi-dvs/V2CE-Toolbox) |
|      EREBUS       |           ArXiv-2025           | [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)                                                               |        [GitHub](https://github.com/apalkk/EBUS)         |


#### Representative Work Using Simulated Datasets
- [TIP-2022] [Asynchronous Spatio-Temporal Memory Network for Continuous Event-Based Object Detection](https://ieeexplore.ieee.org/document/9749022)
- [ICCV-2023] [Learning Optical Flow from Event Camera with Rendered Dataset](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Learning_Optical_Flow_from_Event_Camera_with_Rendered_Dataset_ICCV_2023_paper.pdf)
- [TRO-2024] [CMax-SLAM: Event-Based Rotational-Motion Bundle Adjustment and SLAM System Using Contrast Maximization](https://ieeexplore.ieee.org/abstract/document/10474186)
- [CVPR-2024] [Towards Robust Event-guided Low-Light Image Enhancement: A Large-Scale Real-World Event-Image Dataset and Novel Approach](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_Towards_Robust_Event-guided_Low-Light_Image_Enhancement_A_Large-Scale_Real-World_Event-Image_CVPR_2024_paper.pdf)
- [CVPR-2025] [Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras](https://openaccess.thecvf.com/content/CVPR2025/papers/Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with_CVPR_2025_paper.pdf)


## Hardware Resources
### Neuromorphic Sensors
#### Vision Sensors
|  Sensor  |                       Pub.                        | Paper                                                                                                                                                                               |                                    Link                                    |
|:--------:|:-------------------------------------------------:|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------:|
|   DVS    |     IEEE Journal of Solid-State Circuits-2008     | [A 128 √ó 128 120 dB 15 Œºs Latency Asynchronous Temporal Contrast Vision Sensor](https://ieeexplore.ieee.org/document/4444573)                                                       |                                     -                                      |
|   ATIS   |                    ISSCC-2010                     | [A QVGA 143dB dynamic range asynchronous address-event PWM dynamic image sensor with lossless pixel-level video compression](https://ieeexplore.ieee.org/abstract/document/5433973) |                                     -                                      |
| DAVIS240 |     IEEE Journal of Solid-State Circuits-2014     | [A 240 √ó 180 130 dB 3 ¬µs Latency Global Shutter Spatiotemporal Vision Sensor](https://ieeexplore.ieee.org/document/6889103)                                                         |  [Link](https://docs.inivation.com/_static/hardware_guides/davis240.pdf)   |
| DAVIS346 | IEEE Trans. Circuits Syst. II-Express Briefs-2018 | [Front and Back Illuminated Dynamic and Active Pixel Vision Sensors Comparison](https://ieeexplore.ieee.org/document/8334288)                                                       | [Link](https://docs.inivation.com/hardware/current-products/davis346.html) |
| CeleX-V  |                    CVPRW-2019                     | [Live Demonstration: CeleX-V: A 1M Pixel Multi-Mode Event-Based Sensor](https://ieeexplore.ieee.org/document/9025556)                                                               |                    [Link](https://github.com/CelePixel)                    |
|  Vidar   |                 Engineering-2023                  | [1000√ó Faster Camera and Machine Vision with Ordinary Devices](https://www.sciencedirect.com/science/article/pii/S2095809922002077)                                                 |                                     -                                      |
|  Speck   |                 Nat. Commun.-2024                 | [Spike-based dynamic computing with asynchronous sensing-computing neuromorphic chip](https://www.nature.com/articles/s41467-024-47811-6)                                           |         [Link](https://www.synsense.ai/zh-hans/products/speck-2/)          |
| Tianmouc |                    Nature-2024                    | [A vision chip with complementary pathways for open-world sensing](https://www.nature.com/articles/s41586-024-07358-4)                                                              |                                     -                                      |


| Company | Cameras / Devices                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Softwares/Tools |
|---------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| [iniVation](https://inivation.com/) & [SynSense](https://www.synsense.ai/)| [DAVIS346](https://docs.inivation.com/hardware/current-products/davis346.html), [DAVIS346 AER](https://docs.inivation.com/hardware/current-products/davis346-aer.html), [DVXplorer](https://docs.inivation.com/hardware/current-products/dvxplorer.html), [DVXplorer Lite](https://docs.inivation.com/hardware/current-products/dvxplorer-lite.html), [DVXplorer Micro](https://docs.inivation.com/hardware/current-products/dvxplorer-micro.html), [DVXplorer S Duo](https://docs.inivation.com/hardware/current-products/dvxplorer-s-duo.html), [Stereo Kit](https://docs.inivation.com/hardware/current-products/stereo-kit.html)                                                                                                                               | [Software Solutions](https://docs.inivation.com/software/introduction.html) |
| [Prophesee](https://www.prophesee.ai/) | [Metavision¬Æ sensor GENX320](https://www.prophesee.ai/event-based-sensor-genx320/), [Metavision¬Æ sensor IMX636](https://www.prophesee.ai/event-based-sensor-imx636-sony-prophesee/), [Metavision¬Æ sensor IMX646](https://www.prophesee.ai/event-based-sensor-imx646-sony-prophesee/), [Metavision¬Æ sensor IMX637](https://www.prophesee.ai/event-based-sensor-imx637-sony-prophesee/), [Metavision¬Æ sensor IMX647](https://www.prophesee.ai/event-based-sensor-imx647-sony-prophesee/), </br> [GenX320 ‚Äì CM2](https://www.prophesee.ai/optical-flex-module-genx320-info/), [GenX320 ‚Äì 4√ó5](https://www.prophesee.ai/ca4x5-genx320-info/), [GenX320 ‚Äì CN1](https://www.prophesee.ai/cn1-genx320-info/), [GenX320 ‚Äì CN3](https://www.prophesee.ai/cn3-genx320-info/) | [Metavision SDK](https://docs.prophesee.ai/stable/index.html) |
| [Lucid Vision Labs](https://thinklucid.com/triton2-evs/) | [Triton2 EVS 0.9 MP Model (IMX636)](https://thinklucid.com/product/triton2-evs-0-9mp-imx636/), [Triton2 EVS 0.3 MP Model (IMX637)](https://thinklucid.com/product/triton2-evs-0-3mp-imx637/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | ‚Äî |
| [IDS Imaging](https://en.ids-imaging.com/ueye-evs-cameras.html) | [¬µEye XCP-E UE-39B0XCP-E (IMX636)](https://en.ids-imaging.com/store/ue-39b0xcp-e.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | [Metavision SDK](https://docs.prophesee.ai/stable/index.html) (‚â• 4.6.2) |

#### Auditory Sensors
- AER EAR: A Matched Silicon Cochlea Pair With Address Event Representation Interface [[Paper](https://ieeexplore.ieee.org/document/4061005)]
- Asynchronous binaural spatial audition sensor with 2 √ó 64 √ó 4 channel output [[Paper](https://ieeexplore.ieee.org/document/6658899)] [[Link](https://inilabs.com/products/dynamic-audio-sensor/)]
- A 0.5V 55ŒºW 64√ó2-channel binaural silicon cochlea for event-driven stereo-audio sensing [[Paper](https://ieeexplore.ieee.org/document/7418070)]
- Neuromorphic acoustic sensing using an adaptive microelectromechanical cochlea with integrated feedback [[Paper](https://www.nature.com/articles/s41928-023-00957-5)]

#### Tactile Sensors
- NeuroTac: A Neuromorphic Optical Tactile Sensor applied to Texture Recognition [[Paper](https://ieeexplore.ieee.org/document/9197046/)]
- A spiking and adapting tactile sensor for neuromorphic applications [[Paper](https://www.nature.com/articles/s41598-020-74219-1)]
- An artificial piezotronic synapse for tactile perception [[Paper](https://www.sciencedirect.com/science/article/pii/S221128552030313X)]
- Neuromorphic capacitive tactile sensors inspired by slowly adaptive mechanoreceptors [[Paper](https://ieeexplore.ieee.org/document/9841988)]
- Neuromorphic antennal sensory system [[Paper](https://www.nature.com/articles/s41467-024-46393-7)]

#### Olfactory Sensors
- Analog VLSI Circuit Implementation of an Adaptive Neuromorphic Olfaction Chip [[Paper](https://ieeexplore.ieee.org/document/4061019)]
- Artificial Olfactory Neuron for an In-Sensor Neuromorphic Nose [[Paper](https://advanced.onlinelibrary.wiley.com/doi/10.1002/advs.202106017)]
- A Neuromorphic Electronic Nose Design [[Paper](https://ieeexplore.ieee.org/document/10994250)]
- A bio-inspired neuromorphic olfaction system for highly sensitive and selective gas sensing at room temperature [[Paper](https://www.sciencedirect.com/science/article/pii/S0925400525009748)]
- Gas localization and tracking using neuromorphic olfactory circuit with coupled sensors and memristive neurons [[Paper](https://www.cell.com/device/fulltext/S2666-9986(25)00205-4)]
- Neuromorphic olfaction with ultralow-power gas sensors and ovonic threshold switch [[Paper](https://www.science.org/doi/full/10.1126/sciadv.adv9222)]

#### Wireless Sensors
- NeuroRadar: A Neuromorphic Radar Sensor for Low-Power IoT Systems [[Paper](https://dl.acm.org/doi/10.1145/3625687.3625788)]
- RF neuromorphic spiking sensor for smart IoT devices [[Paper](https://link.springer.com/article/10.1007/s10470-023-02164-w)]
- Neuromorphic Wireless Cognition: Event-Driven Semantic Communications for Remote Inference [[Paper](https://ieeexplore.ieee.org/document/10016643)]
- Neuromorphic Integrated Sensing and Communications [[Paper](https://ieeexplore.ieee.org/document/9997098)]


### Neuromorphic Processors
|                                                  Chips                                                  | Signals | On-chip learning | Process (nm) | Neurons / Synapses | Power (mW) | Energy/SOP (pJ) |                                            Software                                            |
|:-------------------------------------------------------------------------------------------------------:|:-------:|:----------------:|:------------:|:------------------:|:----------:|:---------------:|:----------------------------------------------------------------------------------------------:|
|                        [Neurogrid](https://ieeexplore.ieee.org/document/6805187)                        |  Mixed  |        No        |     180      |    64 K / 100 M    |    2700    |       119       |          [NEF](https://web.stanford.edu/group/brainsinsilicon/blog/computation_nef/)           |
|                        [Braindrop](https://ieeexplore.ieee.org/document/8591981)                        |  Mixed  |       Yes        |      28      |     4 K / 16 M     |   N. A.    |      0.38       |                                             Custom                                             |
| [BrainScaleS](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2011.00117/full) |  Mixed  |       Yes        |     180      |    512 / 128 K     |   N. A.    |       100       |                                         BrainScaleS OS                                         |
|                      [BrainScaleS2](https://ieeexplore.ieee.org/document/9896927)                       |  Mixed  |       Yes        |      65      |    512 / 131 K     |   ~1000    |      N. A.      |                                         BrainScaleS OS                                         |
|    [ROLLS](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2015.00141/full)    |  Mixed  |       Yes        |     180      |     256 / 64 K     |     ~5     |      N. A.      |                                         Custom python                                          |
|  [Dynap-SE](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2018.00891/full)   |  Mixed  |        No        |     180      |    1024 / 65 K     |     ~5     |       260       |                                         Custom python                                          |
|                [Dynap-SE2](https://iopscience.iop.org/article/10.1088/2634-4386/ad1cd7)                 |  Mixed  |       Yes        |     180      |    1024 / 65 K     |   N. A.    |       150       |              [Samna](https://synsense-sys-int.gitlab.io/samna/0.48.2/index.html)               |
|                          [Loihi](https://ieeexplore.ieee.org/document/8259423)                          | Digital |       Yes        |      14      |   128 K / 128 M    |   ~1000    |      23.6       |                            [Lava](https://github.com/lava-nc/lava)                             |
|                         [Loihi2](https://ieeexplore.ieee.org/document/9605018/)                         | Digital |       Yes        |      7       |    1 M / 120 M     |    <50     |      N. A.      |                            [Lava](https://github.com/lava-nc/lava)                             |
|                       [TrueNorth](https://ieeexplore.ieee.org/document/7229264/)                        | Digital |        No        |      28      |    1 M / 256 M     |    ~70     |       26        |                                             Custom                                             |
|                    [NorthPole](https://www.science.org/doi/10.1126/science.adh1174)                     | Digital |        No        |      12      |       N. A.        |   N. A.    |      N. A.      |                                             Custom                                             |
|                        [SpiNNaker](https://ieeexplore.ieee.org/document/6515159)                        | Digital |       Yes        |     130      |    18 K / 18 M     |   ~1000    |      4000       | [PyNN](https://github.com/NeuralEnsemble/PyNN), [NEST](https://github.com/nest/nest-simulator) |
|                             [SpiNNaker2](https://arxiv.org/abs/1911.02385)                              | Digital |       Yes        |      22      |   Configuration    |   ~1000    |       10        | [PyNN](https://github.com/NeuralEnsemble/PyNN), [NEST](https://github.com/nest/nest-simulator) |
|                      [Tianjic](https://www.nature.com/articles/s41586-019-1424-8)                       | Digital |        No        |      28      |    40 K / 10 M     |   ~1000    |      1.54       |                                             Custom                                             |
|                   [TianjicX](https://www.science.org/doi/10.1126/scirobotics.abk2948)                   | Digital |       Yes        |      28      |    160 K / 20 M    |    ~600    |      N. A.      |                                             Custom                                             |
|                  [Darwin](https://link.springer.com/article/10.1007/s11432-015-5511-7)                  | Digital |        No        |     180      |   2048 / 4.19 M    |    0.84    |      N. A.      |                                           Darwin OS                                            |
|                                                 Darwin2                                                 | Digital |        No        |      55      |    147 k / 10 M    |    ~100    |      N. A.      |                                           Darwin OS                                            |
|                  [Darwin3](https://academic.oup.com/nsr/article/11/5/nwae102/7631347)                   | Digital |       Yes        |      22      |     2.3 M / ‚Äî      |   N. A.    |      5.47       |                                           Darwin OS                                            |


## Software Resources